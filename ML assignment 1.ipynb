{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The three stages to build the hypotheses or model in machine learning\n",
    "\n",
    "Model building\n",
    "\n",
    "Model testing\n",
    "\n",
    "Applying the mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the standard approach to supervised learning?\n",
    "\n",
    "There are three types of Machine learning models viz., Supervised learning, unsupervised learning & reinforcement learning, classified based upon the nature of dataset and the features & labels available in the dataset.\n",
    "\n",
    "Supervised learning invloves building the machine learning model using the given features and labels which means machine is trained with the already available set of inputs and the corresponding outputs.\n",
    "\n",
    "The standard approach to supervised learning is to split the set of example into the training data set and the test data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  What is Training set and Test set? \n",
    "\n",
    "In supervised machine learning model, the given dataset is divided into two parts viz., training dataset and test dataset. \n",
    "\n",
    "Training dataset is a set of data is used to discover the potentially predictive relationship between the features and labels that is the inputs and output. Training set comprises examples given to the learner, while Test set is used to test the accuracy of the hypotheses generated by the learner, and it is the set of example held back from the learner.\n",
    "Training set are distinct from Test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  What is the general principle of an ensemble method and what is bagging       and boosting in ensemble method? \n",
    "Ensemble methods aim at improving the predictive performance of a given\n",
    "statistical learning or model ﬁtting technique. The general principle of en-\n",
    "semble methods is to construct a linear combination of some model ﬁtting\n",
    "method, instead of using a single ﬁt of the method.\n",
    "\n",
    "Ensemble methods became popular as a relatively simple device to improve\n",
    "the predictive performance of a base procedure. There are diﬀerent reasons\n",
    "for this: the bagging procedure turns out to be a variance reduction scheme,\n",
    "at least for some base procedures. On the other hand, boosting methods are\n",
    "primarily reducing the (model) bias of the base procedure. This already in-\n",
    "dicates that bagging and boosting are very diﬀerent ensemble methods.\n",
    "\n",
    "Bagging, a sobriquet for bootstrap aggregating, is an ensemble\n",
    "method for improving unstable estimation or classiﬁcation schemes. Breiman\n",
    "motivated bagging as a variance reduction technique for a given base\n",
    "procedure, such as decision trees or methods that do variable selection and\n",
    "ﬁtting in a linear model. It has attracted much attention, probably due to\n",
    "its implementational simplicity and the popularity of the bootstrap method-\n",
    "ology. At the time of its invention, only heuristic arguments were presented\n",
    "why bagging would work. Later, it has been shown in that bagging\n",
    "is a smoothing operation which turns out to be advantageous when aiming to\n",
    "improve the predictive performance of regression or classiﬁcation trees. In\n",
    "case of decision trees, the theory in conﬁrms Breiman’s intuition that\n",
    "bagging is a variance reduction technique, reducing also the mean squared\n",
    "error.\n",
    "\n",
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.\n",
    "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n",
    "AdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting.Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can you avoid overfitting?\n",
    "\n",
    "Overfitting” is traditionally defined as training some flexible representation so that it memorizes the data but fails to predict well in the future. For this post, I will define overfitting more generally as over-representing the performance of systems. There are two styles of general overfitting: over-representing performance on particular datasets and (implicitly) over-representing performance of a method on future datasets. \n",
    "\n",
    "The simplest way to avoid over-fitting is to make sure that the number of independent parameters in your fit is much smaller than the number of data points you have.  By independent parameters, I mean the number of coefficients in a polynomial or the number of weights and biases in a neural network, not the number of independent variables.  The rule-of-thumb is to select a form for the fit such that the number of data points is 5X to 10X the number of coefficients.  If you cannot afford the luxury, you can go lower never below 2X.  Simple example: If you have ten data points in a single variable, y=f(x), a 9th order polynomial will give you a perfect fit -- classic example of over-fitting.  Using my rule-of-thumb, you would try to fit a quadratic or a fourth-order curve.  The basic idea is that if the number of data points is ten times the number of parameters, overfitting is not possible.\n",
    "\n",
    "The \"classic\" way to avoid overfitting is to divide your data sets into three groups -- a training set, a test set, and a validation set.  You find the coefficients using the training set;  you find the best form of the equation using the test set, test for over-fitting using the validation set.  Be careful not to use the validation set until after you have picked the best form of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
